seed: 42
dataset: qm9
output_dir: runs/qm9

# Training
num_epochs: 100
batch_size: 256            # Lower this if GPU memory is limited and use grad_accumulation
grad_accumulation_steps: 1 # Increase this (e.g., 4 or 8) if you reduce batch_size
learning_rate: 3.0e-4
weight_decay: 0.0
optimizer: adamw
lr_scheduler:
  name: cosine            # Options: constant, cosine
  warmup_steps: 500       # Set to 0 if no warmup needed

precision:
  amp: true               # Mixed precision on GPU
  grad_clip_norm: 1.0

num_workers: 4

# Data / Molecule representation settings
data:
  include_hydrogens: false         # Confirm: most works on QM9 exclude H (like DiGress)
  atom_types: [C, N, O, F]         # If keeping H: [C, N, O, F, H]
  bond_types: [single, double, triple, aromatic]
  max_nodes: 9                     # Without H usually 9 heavy atoms; increase if keeping H
  sanitize_rdkit: true
  random_split: false              # Most works use standard QM9 split

# Diffusion / Model settings
diffusion:
  time_continuous: true
  num_sample_steps: 1000           # Number of sampling steps; change if paper specifies
  ema_decay: 0.999
  rate_schedule: cosine            # Placeholders: cosine, linear
  min_time: 1.0e-4
  max_time: 1.0

model:
  hidden_dim: 256
  num_layers: 8
  dropout: 0.0
  norm: layer                      # Set according to paper if specific normalization
  activation: gelu                 # Set according to paper if specific activation

# Checkpoint / Resume
checkpointing:
  checkpoint_dir: checkpoints/qm9
  save_every_steps: 2000
  keep_last_k: 5
  resume: true
  resume_from: ""
  save_on_interrupt: true
  autosave_minutes: 10

# Logging
logging:
  log_every_steps: 50
  use_tqdm: true

# Evaluation / Sampling
evaluation:
  eval_every_epochs: 1
  num_generate_samples: 1000
  save_generated_smiles: true
  generated_path: outputs/qm9/samples.smi
