# Molecular Generation Evaluation Pipeline

A comprehensive evaluation pipeline for molecules generated by diffusion models on the QM9 dataset (without hydrogens).

## 📋 Overview

This pipeline computes **9 key metrics** for evaluating generated molecules:

1. **Validity** - Fraction of chemically valid molecules
2. **Uniqueness** - Fraction of unique molecules among valid ones
3. **Novelty** - Fraction of molecules not in training set
4. **FCD** (Fréchet ChemNet Distance) - Distribution similarity
5. **Atom Stability** - Fraction of atoms with valid configurations
6. **Mol Stability** - Fraction of stable molecules
7. **MMD** (Maximum Mean Discrepancy) - Distribution distance
8. **NLL** (Negative Log-Likelihood) - Likelihood estimation
9. **NSPDK** (Neighborhood Subgraph Pairwise Distance Kernel) - Graph similarity

---

## 🚀 Quick Start

### Installation

```bash
pip install -r requirements_metrics.txt
```

### Basic Usage

```bash
python evaluate_metrics.py --generated generated.smi --reference reference.smi
```

### Example with Test Data

```bash
python evaluate_metrics.py --generated example_generated.smi --reference example_reference.smi --output_prefix test_results
```

---

## 📄 Input File Formats

### SMILES Files (.smi or .txt)

One SMILES string per line:

```
CCO
CCC
NCCN
c1ccccc1
CC(=O)O
```

### CSV Files (.csv)

CSV with a SMILES column (named 'smiles', 'SMILES', or first column):

```csv
smiles,id
CCO,1
CCC,2
NCCN,3
```

---

## 📊 Output Files

The script generates two output files:

### 1. JSON Format (`metrics_results.json`)

```json
{
  "validity": {
    "validity": 0.95,
    "valid_count": 950,
    "total_count": 1000
  },
  "uniqueness": {
    "uniqueness": 0.87,
    "unique_count": 826
  },
  "novelty": {
    "novelty": 0.73,
    "novel_count": 603
  },
  ...
}
```

### 2. CSV Format (`metrics_results.csv`)

```csv
metric,submetric,value
validity,validity,0.95
validity,valid_count,950
uniqueness,uniqueness,0.87
...
```

---

## 🔧 Command-Line Options

```bash
python evaluate_metrics.py [OPTIONS]

Required Arguments:
  --generated PATH      Path to generated SMILES file
  --reference PATH      Path to reference SMILES file

Optional Arguments:
  --output_prefix STR   Output file prefix (default: metrics_results)
  --max_samples INT     Limit number of samples for testing
```

---

## 📈 Metric Descriptions

### Core Metrics

- **Validity**: `valid_molecules / total_molecules`
- **Uniqueness**: `unique_valid_molecules / valid_molecules`
- **Novelty**: `molecules_not_in_training / valid_molecules`

### Quality Metrics

- **Atom Stability**: Fraction of atoms with valid configurations (reasonable formal charges)
- **Mol Stability**: Fraction of molecules meeting stability criteria (valid MW, atom count, charges)

### Distribution Metrics

- **FCD**: Measures similarity between generated and reference distributions using ChemNet features
- **MMD**: Kernel-based distance between molecular descriptor distributions
- **NLL**: Negative log-likelihood under multivariate normal approximation
- **NSPDK**: Graph kernel similarity measuring structural similarity

---

## 🧪 Example Workflow

### 1. Prepare Your Data

```bash
# Your generated molecules
cat > my_generated.smi << EOF
CCO
CCC
NCCN
EOF

# Reference molecules (e.g., QM9 test set without H)
cat > qm9_reference.smi << EOF
CCO
CCC
CCCC
CCCCC
EOF
```

### 2. Run Evaluation

```bash
python evaluate_metrics.py \
    --generated my_generated.smi \
    --reference qm9_reference.smi \
    --output_prefix my_experiment
```

### 3. View Results

```bash
# View console output for summary
# Check my_experiment.json for detailed results
# Load my_experiment.csv into pandas/Excel for analysis
```

---

## 📦 Dependencies

### Required
- numpy, pandas, scipy, scikit-learn
- rdkit-pypi (chemistry toolkit)
- torch, torchmetrics

### Optional (for advanced metrics)
- fcd-torch (FCD metric)
- guacamol (molecular benchmarks)
- moses (molecular sets comparisons)
- networkx (graph analysis)

---

## ⚠️ Troubleshooting

### ImportError for Optional Libraries

If you see warnings about missing libraries:
- **FCD**: `pip install fcd-torch`
- **MOSES**: `pip install moses`
- **GuacaMol**: `pip install guacamol`

The script will skip metrics that require unavailable libraries.

### Invalid SMILES

The script handles invalid SMILES gracefully and reports counts in the output.

### Memory Issues

For large datasets, use `--max_samples` to limit evaluation:

```bash
python evaluate_metrics.py --generated large.smi --reference large_ref.smi --max_samples 10000
```

---

## 🔬 Advanced Usage

### Python API

```python
from evaluate_metrics import (
    load_smiles,
    compute_validity,
    compute_uniqueness,
    compute_novelty,
    compute_fcd,
    compute_mmd,
    save_results
)

# Load data
gen_smiles = load_smiles('generated.smi')
ref_smiles = load_smiles('reference.smi')

# Compute specific metrics
validity = compute_validity(gen_smiles)
uniqueness = compute_uniqueness(gen_smiles)
novelty = compute_novelty(gen_smiles, ref_smiles)

# Save results
metrics = {
    'validity': validity,
    'uniqueness': uniqueness,
    'novelty': novelty
}
save_results(metrics, 'custom_results')
```

---

## 📚 References

- **QM9 Dataset**: Ramakrishnan et al., *Scientific Data* 2014
- **FCD**: Preuer et al., *J. Chem. Inf. Model.* 2018
- **MMD**: Gretton et al., *JMLR* 2012
- **NSPDK**: Costa & De Grave, *BMC Bioinformatics* 2010

---

## 📝 Citation

If you use this evaluation pipeline, please cite:

```bibtex
@misc{qm9_evaluation,
  title={Molecular Generation Evaluation Pipeline for QM9},
  author={Your Name},
  year={2025}
}
```

---

## 📞 Support

For issues or questions:
- Check existing QM9 dataset papers for metric definitions
- Verify SMILES format matches QM9 (no explicit hydrogens)
- Ensure reference dataset is from QM9 test/validation set

---

## ✅ Checklist for Use

- [ ] Install dependencies: `pip install -r requirements_metrics.txt`
- [ ] Prepare generated.smi file (one SMILES per line, no hydrogens)
- [ ] Prepare reference.smi file (QM9 molecules, no hydrogens)
- [ ] Run evaluation: `python evaluate_metrics.py --generated X --reference Y`
- [ ] Check output files: `metrics_results.json` and `metrics_results.csv`
- [ ] Analyze results and compare with baselines

---

**Good luck with your molecular generation experiments! 🚀**

