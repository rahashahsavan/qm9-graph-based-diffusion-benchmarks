#!/usr/bin/env python3
"""
Comprehensive evaluation script for molecules generated by diffusion models on QM9 dataset (without hydrogens).

This script computes 9 key metrics:
1. Novelty
2. FCD (Fréchet ChemNet Distance)
3. Validity
4. Uniqueness
5. Atom Stability
6. Mol Stability
7. Maximum Mean Discrepancy (MMD)
8. NLL (Negative Log-Likelihood)
9. NSPDK (Neighborhood Subgraph Pairwise Distance Kernel)

Usage:
    python evaluate_metrics.py --generated generated.smi --reference reference.smi
"""

import os
import sys
import json
import csv
import argparse
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Core dependencies
try:
    import torch
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    print("Warning: PyTorch not available. Some metrics will be skipped.")
    TORCH_AVAILABLE = False

try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors, Crippen, QED, rdMolDescriptors
    from rdkit.Chem.Scaffolds import MurckoScaffold
    RDKIT_AVAILABLE = True
except ImportError:
    print("Warning: RDKit not available. Some metrics will be skipped.")
    RDKIT_AVAILABLE = False

try:
    from sklearn.metrics import pairwise_distances
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    print("Warning: scikit-learn not available. Some metrics will be skipped.")
    SKLEARN_AVAILABLE = False

try:
    import scipy.stats as stats
    from scipy.spatial.distance import pdist, squareform
    SCIPY_AVAILABLE = True
except ImportError:
    print("Warning: SciPy not available. Some metrics will be skipped.")
    SCIPY_AVAILABLE = False

# Optional dependencies for advanced metrics
try:
    import fcd_torch
    FCD_AVAILABLE = True
except ImportError:
    print("Warning: fcd_torch not available. FCD metric will be skipped.")
    FCD_AVAILABLE = False

try:
    from guacamol import assess_distribution_learning
    GUACAMOL_AVAILABLE = True
except ImportError:
    print("Warning: guacamol not available. Some metrics will be skipped.")
    GUACAMOL_AVAILABLE = False

try:
    import moses
    MOSES_AVAILABLE = True
except ImportError:
    print("Warning: MOSES not available. Some metrics will be skipped.")
    MOSES_AVAILABLE = False


def load_smiles(file_path: str) -> List[str]:
    """Load SMILES from file (supports .smi, .txt, .csv formats)."""
    print(f"Loading SMILES from: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    
    smiles_list = []
    
    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
        # Try common column names for SMILES
        smiles_col = None
        for col in ['smiles', 'SMILES', 'smile', 'SMILE']:
            if col in df.columns:
                smiles_col = col
                break
        if smiles_col is None:
            smiles_col = df.columns[0]  # Use first column
        smiles_list = df[smiles_col].dropna().tolist()
    else:
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    smiles_list.append(line)
    
    print(f"Loaded {len(smiles_list)} SMILES")
    return smiles_list


def compute_validity(smiles_list: List[str]) -> Dict[str, Any]:
    """Compute validity metric."""
    print("Computing validity...")
    
    if not RDKIT_AVAILABLE:
        return {'validity': None, 'valid_count': 0, 'total_count': len(smiles_list)}
    
    valid_smiles = []
    invalid_count = 0
    
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                Chem.SanitizeMol(mol)
                valid_smiles.append(smiles)
            else:
                invalid_count += 1
        except:
            invalid_count += 1
    
    validity = len(valid_smiles) / len(smiles_list) if smiles_list else 0
    
    return {
        'validity': validity,
        'valid_count': len(valid_smiles),
        'invalid_count': invalid_count,
        'total_count': len(smiles_list)
    }


def compute_uniqueness(smiles_list: List[str]) -> Dict[str, Any]:
    """Compute uniqueness metric."""
    print("Computing uniqueness...")
    
    if not RDKIT_AVAILABLE:
        return {'uniqueness': None, 'unique_count': 0, 'total_count': len(smiles_list)}
    
    # First get valid SMILES
    valid_smiles = []
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                Chem.SanitizeMol(mol)
                valid_smiles.append(smiles)
        except:
            continue
    
    if not valid_smiles:
        return {'uniqueness': 0.0, 'unique_count': 0, 'total_count': len(smiles_list)}
    
    unique_smiles = list(set(valid_smiles))
    uniqueness = len(unique_smiles) / len(valid_smiles)
    
    return {
        'uniqueness': uniqueness,
        'unique_count': len(unique_smiles),
        'valid_count': len(valid_smiles),
        'total_count': len(smiles_list)
    }


def compute_novelty(generated_smiles: List[str], reference_smiles: List[str]) -> Dict[str, Any]:
    """Compute novelty metric."""
    print("Computing novelty...")
    
    if not reference_smiles:
        return {'novelty': None, 'novel_count': 0, 'total_count': len(generated_smiles)}
    
    if not RDKIT_AVAILABLE:
        return {'novelty': None, 'novel_count': 0, 'total_count': len(generated_smiles)}
    
    # Get valid generated SMILES
    valid_generated = []
    for smiles in generated_smiles:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                Chem.SanitizeMol(mol)
                valid_generated.append(smiles)
        except:
            continue
    
    if not valid_generated:
        return {'novelty': 0.0, 'novel_count': 0, 'total_count': len(generated_smiles)}
    
    # Create reference set
    reference_set = set(reference_smiles)
    
    # Count novel molecules
    novel_count = sum(1 for smiles in valid_generated if smiles not in reference_set)
    novelty = novel_count / len(valid_generated)
    
    return {
        'novelty': novelty,
        'novel_count': novel_count,
        'valid_generated_count': len(valid_generated),
        'total_count': len(generated_smiles)
    }


def compute_fcd(generated_smiles: List[str], reference_smiles: List[str]) -> Dict[str, Any]:
    """Compute Fréchet ChemNet Distance."""
    print("Computing FCD...")
    
    if not FCD_AVAILABLE or not RDKIT_AVAILABLE:
        return {'fcd': None}
    
    if not reference_smiles:
        return {'fcd': None}
    
    try:
        # Filter valid SMILES
        valid_generated = []
        valid_reference = []
        
        for smiles in generated_smiles:
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is not None:
                    Chem.SanitizeMol(mol)
                    valid_generated.append(smiles)
            except:
                continue
        
        for smiles in reference_smiles:
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is not None:
                    Chem.SanitizeMol(mol)
                    valid_reference.append(smiles)
            except:
                continue
        
        if not valid_generated or not valid_reference:
            return {'fcd': None}
        
        # Compute FCD
        fcd_score = fcd_torch.fcd(valid_generated, valid_reference)
        
        return {
            'fcd': fcd_score,
            'valid_generated_count': len(valid_generated),
            'valid_reference_count': len(valid_reference)
        }
    
    except Exception as e:
        print(f"Error computing FCD: {e}")
        return {'fcd': None}


def compute_atom_stability(smiles_list: List[str]) -> Dict[str, Any]:
    """Compute atom stability metric."""
    print("Computing atom stability...")
    
    if not RDKIT_AVAILABLE:
        return {'atom_stability': None}
    
    stable_count = 0
    total_atoms = 0
    
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                Chem.SanitizeMol(mol)
                
                # Check each atom
                for atom in mol.GetAtoms():
                    total_atoms += 1
                    # Simple stability check: valid atom type and reasonable formal charge
                    if atom.GetAtomicNum() > 0 and abs(atom.GetFormalCharge()) <= 3:
                        stable_count += 1
        
        except:
            continue
    
    atom_stability = stable_count / total_atoms if total_atoms > 0 else 0
    
    return {
        'atom_stability': atom_stability,
        'stable_atoms': stable_count,
        'total_atoms': total_atoms
    }


def compute_mol_stability(smiles_list: List[str]) -> Dict[str, Any]:
    """Compute molecular stability metric."""
    print("Computing molecular stability...")
    
    if not RDKIT_AVAILABLE:
        return {'mol_stability': None}
    
    stable_count = 0
    total_mols = 0
    
    for smiles in smiles_list:
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol is not None:
                Chem.SanitizeMol(mol)
                total_mols += 1
                
                # Check molecular stability criteria
                is_stable = True
                
                # Check for reasonable molecular weight
                mw = Descriptors.MolWt(mol)
                if mw < 10 or mw > 1000:  # Reasonable range for QM9
                    is_stable = False
                
                # Check for reasonable number of atoms
                num_atoms = mol.GetNumAtoms()
                if num_atoms < 3 or num_atoms > 50:  # Reasonable range for QM9
                    is_stable = False
                
                # Check for reasonable formal charges
                for atom in mol.GetAtoms():
                    if abs(atom.GetFormalCharge()) > 2:
                        is_stable = False
                        break
                
                if is_stable:
                    stable_count += 1
        
        except:
            continue
    
    mol_stability = stable_count / total_mols if total_mols > 0 else 0
    
    return {
        'mol_stability': mol_stability,
        'stable_molecules': stable_count,
        'total_molecules': total_mols
    }


def compute_mmd(generated_smiles: List[str], reference_smiles: List[str]) -> Dict[str, Any]:
    """Compute Maximum Mean Discrepancy."""
    print("Computing MMD...")
    
    if not SKLEARN_AVAILABLE or not RDKIT_AVAILABLE:
        return {'mmd': None}
    
    if not reference_smiles:
        return {'mmd': None}
    
    try:
        # Extract molecular descriptors
        def get_descriptors(smiles_list):
            descriptors = []
            for smiles in smiles_list:
                try:
                    mol = Chem.MolFromSmiles(smiles)
                    if mol is not None:
                        Chem.SanitizeMol(mol)
                        desc = [
                            Descriptors.MolWt(mol),
                            Descriptors.NumAtoms(mol),
                            Descriptors.NumBonds(mol),
                            Descriptors.NumRotatableBonds(mol),
                            Descriptors.TPSA(mol),
                            Descriptors.LogP(mol),
                            Descriptors.NumHBD(mol),
                            Descriptors.NumHBA(mol)
                        ]
                        descriptors.append(desc)
                except:
                    continue
            return np.array(descriptors)
        
        gen_desc = get_descriptors(generated_smiles)
        ref_desc = get_descriptors(reference_smiles)
        
        if len(gen_desc) == 0 or len(ref_desc) == 0:
            return {'mmd': None}
        
        # Standardize features
        scaler = StandardScaler()
        all_desc = np.vstack([gen_desc, ref_desc])
        all_desc_scaled = scaler.fit_transform(all_desc)
        
        gen_desc_scaled = all_desc_scaled[:len(gen_desc)]
        ref_desc_scaled = all_desc_scaled[len(gen_desc):]
        
        # Compute MMD using RBF kernel
        def rbf_kernel(X, Y, gamma=1.0):
            pairwise_dists = pairwise_distances(X, Y, metric='euclidean')
            return np.exp(-gamma * pairwise_dists**2)
        
        gamma = 1.0 / gen_desc_scaled.shape[1]  # 1/n_features
        
        K_XX = rbf_kernel(gen_desc_scaled, gen_desc_scaled, gamma)
        K_YY = rbf_kernel(ref_desc_scaled, ref_desc_scaled, gamma)
        K_XY = rbf_kernel(gen_desc_scaled, ref_desc_scaled, gamma)
        
        mmd = (K_XX.mean() + K_YY.mean() - 2 * K_XY.mean())
        
        return {
            'mmd': mmd,
            'generated_samples': len(gen_desc),
            'reference_samples': len(ref_desc)
        }
    
    except Exception as e:
        print(f"Error computing MMD: {e}")
        return {'mmd': None}


def compute_nll(smiles_list: List[str]) -> Dict[str, Any]:
    """Compute Negative Log-Likelihood (simplified version)."""
    print("Computing NLL...")
    
    if not RDKIT_AVAILABLE:
        return {'nll': None}
    
    try:
        # Simple NLL based on molecular descriptors
        descriptors = []
        for smiles in smiles_list:
            try:
                mol = Chem.MolFromSmiles(smiles)
                if mol is not None:
                    Chem.SanitizeMol(mol)
                    desc = [
                        Descriptors.MolWt(mol),
                        Descriptors.NumAtoms(mol),
                        Descriptors.LogP(mol),
                        Descriptors.TPSA(mol)
                    ]
                    descriptors.append(desc)
            except:
                continue
        
        if len(descriptors) < 2:
            return {'nll': None}
        
        descriptors = np.array(descriptors)
        
        # Compute log-likelihood using multivariate normal approximation
        mean_desc = np.mean(descriptors, axis=0)
        cov_desc = np.cov(descriptors.T)
        
        # Add small regularization to avoid singular matrix
        cov_desc += np.eye(cov_desc.shape[0]) * 1e-6
        
        try:
            inv_cov = np.linalg.inv(cov_desc)
            det_cov = np.linalg.det(cov_desc)
            
            if det_cov <= 0:
                return {'nll': None}
            
            # Compute log-likelihood
            nll = 0
            for desc in descriptors:
                diff = desc - mean_desc
                nll += 0.5 * (np.log(det_cov) + diff.T @ inv_cov @ diff + len(desc) * np.log(2 * np.pi))
            
            nll = nll / len(descriptors)
            
            return {
                'nll': nll,
                'sample_count': len(descriptors)
            }
        
        except np.linalg.LinAlgError:
            return {'nll': None}
    
    except Exception as e:
        print(f"Error computing NLL: {e}")
        return {'nll': None}


def compute_nspdk(generated_smiles: List[str], reference_smiles: List[str]) -> Dict[str, Any]:
    """Compute Neighborhood Subgraph Pairwise Distance Kernel."""
    print("Computing NSPDK...")
    
    if not RDKIT_AVAILABLE or not SKLEARN_AVAILABLE:
        return {'nspdk': None}
    
    if not reference_smiles:
        return {'nspdk': None}
    
    try:
        def smiles_to_features(smiles_list):
            features = []
            for smiles in smiles_list:
                try:
                    mol = Chem.MolFromSmiles(smiles)
                    if mol is not None:
                        Chem.SanitizeMol(mol)
                        
                        # Extract graph-based features
                        num_atoms = mol.GetNumAtoms()
                        num_bonds = mol.GetNumBonds()
                        
                        # Atom type counts
                        atom_counts = Counter([atom.GetAtomicNum() for atom in mol.GetAtoms()])
                        
                        # Bond type counts
                        bond_counts = Counter([bond.GetBondType() for bond in mol.GetBonds()])
                        
                        # Ring information
                        ring_info = rdMolDescriptors.CalcNumRings(mol)
                        
                        # Combine features
                        feature_vector = [
                            num_atoms,
                            num_bonds,
                            ring_info,
                            atom_counts.get(6, 0),  # Carbon
                            atom_counts.get(7, 0),  # Nitrogen
                            atom_counts.get(8, 0),  # Oxygen
                            atom_counts.get(9, 0),  # Fluorine
                            bond_counts.get(Chem.BondType.SINGLE, 0),
                            bond_counts.get(Chem.BondType.DOUBLE, 0),
                            bond_counts.get(Chem.BondType.TRIPLE, 0),
                            bond_counts.get(Chem.BondType.AROMATIC, 0)
                        ]
                        features.append(feature_vector)
                except:
                    continue
            return np.array(features)
        
        gen_features = smiles_to_features(generated_smiles)
        ref_features = smiles_to_features(reference_smiles)
        
        if len(gen_features) == 0 or len(ref_features) == 0:
            return {'nspdk': None}
        
        # Compute kernel matrix
        def compute_kernel_matrix(features1, features2):
            # Simple RBF kernel
            pairwise_dists = pairwise_distances(features1, features2, metric='euclidean')
            gamma = 1.0 / features1.shape[1]
            return np.exp(-gamma * pairwise_dists**2)
        
        K_gen = compute_kernel_matrix(gen_features, gen_features)
        K_ref = compute_kernel_matrix(ref_features, ref_features)
        K_cross = compute_kernel_matrix(gen_features, ref_features)
        
        # Compute NSPDK score (similarity measure)
        nspdk_score = np.mean(K_cross) / (np.sqrt(np.mean(K_gen)) * np.sqrt(np.mean(K_ref)))
        
        return {
            'nspdk': nspdk_score,
            'generated_samples': len(gen_features),
            'reference_samples': len(ref_features)
        }
    
    except Exception as e:
        print(f"Error computing NSPDK: {e}")
        return {'nspdk': None}


def save_results(metrics: Dict[str, Any], output_prefix: str = "metrics_results"):
    """Save results to JSON and CSV files."""
    
    # Save to JSON
    json_file = f"{output_prefix}.json"
    with open(json_file, 'w') as f:
        json.dump(metrics, f, indent=2, default=str)
    print(f"Results saved to {json_file}")
    
    # Save to CSV
    csv_file = f"{output_prefix}.csv"
    csv_data = []
    
    for metric_name, metric_data in metrics.items():
        if isinstance(metric_data, dict):
            for key, value in metric_data.items():
                csv_data.append({
                    'metric': metric_name,
                    'submetric': key,
                    'value': value
                })
        else:
            csv_data.append({
                'metric': metric_name,
                'submetric': 'value',
                'value': metric_data
            })
    
    df = pd.DataFrame(csv_data)
    df.to_csv(csv_file, index=False)
    print(f"Results saved to {csv_file}")


def print_results(metrics: Dict[str, Any]):
    """Print results in a formatted way."""
    print("\n" + "="*60)
    print("MOLECULAR GENERATION EVALUATION RESULTS")
    print("="*60)
    
    # Core metrics
    print("\n📊 CORE METRICS:")
    print("-" * 30)
    
    validity = metrics.get('validity', {})
    print(f"Validity:           {validity.get('validity', 'N/A'):.4f} ({validity.get('valid_count', 0)}/{validity.get('total_count', 0)})")
    
    uniqueness = metrics.get('uniqueness', {})
    print(f"Uniqueness:         {uniqueness.get('uniqueness', 'N/A'):.4f} ({uniqueness.get('unique_count', 0)}/{uniqueness.get('valid_count', 0)})")
    
    novelty = metrics.get('novelty', {})
    print(f"Novelty:            {novelty.get('novelty', 'N/A'):.4f} ({novelty.get('novel_count', 0)}/{novelty.get('valid_generated_count', 0)})")
    
    # Quality metrics
    print("\n🔬 QUALITY METRICS:")
    print("-" * 30)
    
    atom_stability = metrics.get('atom_stability', {})
    print(f"Atom Stability:     {atom_stability.get('atom_stability', 'N/A'):.4f} ({atom_stability.get('stable_atoms', 0)}/{atom_stability.get('total_atoms', 0)})")
    
    mol_stability = metrics.get('mol_stability', {})
    print(f"Mol Stability:      {mol_stability.get('mol_stability', 'N/A'):.4f} ({mol_stability.get('stable_molecules', 0)}/{mol_stability.get('total_molecules', 0)})")
    
    # Distribution metrics
    print("\n📈 DISTRIBUTION METRICS:")
    print("-" * 30)
    
    fcd = metrics.get('fcd', {})
    print(f"FCD:                {fcd.get('fcd', 'N/A'):.4f}")
    
    mmd = metrics.get('mmd', {})
    print(f"MMD:                {mmd.get('mmd', 'N/A'):.4f}")
    
    nll = metrics.get('nll', {})
    print(f"NLL:                {nll.get('nll', 'N/A'):.4f}")
    
    nspdk = metrics.get('nspdk', {})
    print(f"NSPDK:              {nspdk.get('nspdk', 'N/A'):.4f}")
    
    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description='Evaluate generated molecules against reference dataset')
    parser.add_argument('--generated', type=str, required=True, 
                       help='Path to generated SMILES file (.smi, .txt, or .csv)')
    parser.add_argument('--reference', type=str, required=True,
                       help='Path to reference SMILES file (.smi, .txt, or .csv)')
    parser.add_argument('--output_prefix', type=str, default='metrics_results',
                       help='Output file prefix (default: metrics_results)')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='Maximum number of samples to evaluate (for testing)')
    
    args = parser.parse_args()
    
    # Load data
    try:
        generated_smiles = load_smiles(args.generated)
        reference_smiles = load_smiles(args.reference)
    except Exception as e:
        print(f"Error loading data: {e}")
        return
    
    # Limit samples if specified
    if args.max_samples:
        generated_smiles = generated_smiles[:args.max_samples]
        reference_smiles = reference_smiles[:args.max_samples]
        print(f"Limited to {args.max_samples} samples for evaluation")
    
    print(f"\nEvaluating {len(generated_smiles)} generated molecules against {len(reference_smiles)} reference molecules")
    
    # Compute all metrics
    metrics = {}
    
    # Core metrics
    metrics['validity'] = compute_validity(generated_smiles)
    metrics['uniqueness'] = compute_uniqueness(generated_smiles)
    metrics['novelty'] = compute_novelty(generated_smiles, reference_smiles)
    
    # Quality metrics
    metrics['atom_stability'] = compute_atom_stability(generated_smiles)
    metrics['mol_stability'] = compute_mol_stability(generated_smiles)
    
    # Distribution metrics
    metrics['fcd'] = compute_fcd(generated_smiles, reference_smiles)
    metrics['mmd'] = compute_mmd(generated_smiles, reference_smiles)
    metrics['nll'] = compute_nll(generated_smiles)
    metrics['nspdk'] = compute_nspdk(generated_smiles, reference_smiles)
    
    # Save and print results
    save_results(metrics, args.output_prefix)
    print_results(metrics)


if __name__ == '__main__':
    main()
